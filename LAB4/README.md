UCS645 – Parallel & Distributed Computing
Assignment 4 – Introduction to MPI

Name:
Student ID:
Instructor: Saif Nalband

1. Introduction

Message Passing Interface (MPI) is a standardized communication protocol used for parallel programming on distributed memory systems. It allows multiple processes to communicate using message passing techniques. MPI is widely used in high-performance computing because of its scalability, portability, and efficiency.

In this assignment, different MPI programs were implemented to understand point-to-point communication, collective communication, reduction operations, and performance analysis.

2. Experimental Setup

Language: C

MPI Library: OpenMPI

OS: Ubuntu (WSL)

Compiler: mpicc

Execution Command:

mpicc -o program program.c
mpirun -np 4 ./program

Programs were tested with different process counts (1, 2, 4, 8).

3. Exercise 1 – Ring Communication
Objective

To implement a ring topology where each process sends a token to the next process.

Results
Processes	Final Value at Rank 0
2	101
4	106
8	128
Explanation

The token starts at value 100 in process 0. Each process receives the token, adds its rank, and passes it to the next process. The last process sends the token back to process 0.

Inference

The final value increases as the number of processes increases because more ranks contribute to the sum. This confirms correct implementation of ring communication.

4. Exercise 2 – Parallel Array Sum
Objective

To compute the sum of numbers from 1 to 100 using parallel processing.

Results
Processes	Global Sum	Average
1	5050	50.50
2	5050	50.50
4	5050	50.50
Explanation

The array was divided among processes using MPI_Scatter. Each process computed a local sum. MPI_Reduce combined all local sums to produce the final global sum.

Inference

The result remained 5050 for all process counts, verifying correct use of scatter and reduction operations.

5. Exercise 3 – Global Maximum and Minimum
Objective

To find the global maximum and minimum values generated by all processes.

Sample Results
Processes	Global Max	Rank	Global Min	Rank
4	896	2	34	2
8	990	7	18	0
Explanation

Each process generated random numbers and calculated its local minimum and maximum. MPI_MAXLOC and MPI_MINLOC were used to determine global extreme values along with the rank of the process that generated them.

Inference

The program correctly identified global extreme values and their corresponding process ranks using reduction operations.

6. Exercise 4 – Parallel Dot Product
Objective

To compute the dot product of two vectors in parallel.

Vector A = [1,2,3,4,5,6,7,8]
Vector B = [8,7,6,5,4,3,2,1]

Expected Result = 120

Results
Processes	Dot Product
1	120
2	120
4	120
8	120
Explanation

The vectors were divided among processes using MPI_Scatter. Each process computed a partial dot product. MPI_Reduce was used to sum partial results into the final result.

Inference

The final dot product remained 120 for all process counts, proving correctness of parallel computation.

7. Performance Analysis

Execution time was measured using MPI_Wtime(). Performance metrics were calculated as follows:

Speedup:

Sp = T1 / Tp

Efficiency:

Ep = Sp / p
Sample Performance Table
Processes (p)	Time (Tp)	Speedup (Sp)	Efficiency (Ep)
1	T1	1.0	1.0
2	T2	T1/T2	(T1/T2)/2
4	T4	T1/T4	(T1/T4)/4
8	T8	T1/T8	(T1/T8)/8
8. Discussion

The execution time decreased as the number of processes increased. However, speedup was not perfectly linear due to communication overhead and synchronization costs. As the number of processes increases, message passing and coordination overhead increase, reducing efficiency.

Efficiency decreases at higher process counts because communication time becomes significant compared to computation time.

9. Conclusion

This assignment demonstrated the use of MPI for parallel programming. Various communication methods such as point-to-point communication, scatter, reduce, and collective operations were implemented successfully. Performance analysis showed that parallel programs can reduce execution time but are limited by communication overhead and synchronization delays.

# ASSIGNMENT - 5
## REPORT


### Introduction

This assignment investigates the principles of parallel and distributed computing using the Message Passing Interface (MPI). The objective is to understand how computational problems behave under parallel execution and to evaluate scalability, communication overhead, synchronization cost, and load balancing strategies.

### Theoretical Background
2.1 Parallel Speedup

Speedup is defined as:

ğ‘†
(
ğ‘
)
=
ğ‘‡
1
ğ‘‡
ğ‘
S(p)=
T
p
	â€‹

T
1
	â€‹

	â€‹


Where:

ğ‘‡
1
T
1
	â€‹

 = serial execution time

ğ‘‡
ğ‘
T
p
	â€‹

 = parallel execution time with p processes

Ideal speedup is linear:

ğ‘†
(
ğ‘
)
=
ğ‘
S(p)=p

However, in real systems, speedup is limited by communication and synchronization overhead.

2.2 Efficiency
ğ¸
(
ğ‘
)
=
ğ‘†
(
ğ‘
)
ğ‘
E(p)=
p
S(p)
	â€‹


Efficiency measures how effectively processes are utilized.

2.3 Amdahlâ€™s Law

Amdahlâ€™s Law states that the maximum theoretical speedup is limited by the serial portion of a program:

ğ‘†
(
ğ‘
)
=
1
ğ‘“
+
1
âˆ’
ğ‘“
ğ‘
S(p)=
f+
p
1âˆ’f
	â€‹

1
	â€‹


Where:

ğ‘“
f = fraction of serial work

This explains why speedup eventually saturates as processes increase.

3. Theoretical Discussion per Problem
Q1 â€“ DAXPY (Data Parallelism)
Computational Nature

Regular data parallel problem.

Requires data distribution (Scatter) and collection (Gather).

Theoretical Behavior

The total execution time can be modeled as:

ğ‘‡
ğ‘
=
ğ‘‡
ğ‘ 
ğ‘
ğ‘
ğ‘¡
ğ‘¡
ğ‘’
ğ‘Ÿ
+
ğ‘‡
ğ‘
ğ‘œ
ğ‘š
ğ‘
ğ‘¢
ğ‘¡
ğ‘’
+
ğ‘‡
ğ‘”
ğ‘
ğ‘¡
â„
ğ‘’
ğ‘Ÿ
T
p
	â€‹

=T
scatter
	â€‹

+T
compute
	â€‹

+T
gather
	â€‹


Even though computation reduces proportionally to 
1
/
ğ‘
1/p, communication cost does not reduce.

Scalability Analysis

If computation time is small compared to communication, speedup will be limited.

For small vector sizes, communication dominates.

For very large vectors, computation dominates, improving scalability.

Conclusion

DAXPY demonstrates that parallel efficiency strongly depends on the compute-to-communication ratio.

Q2 â€“ Broadcast Race
Manual Broadcast

Manual implementation performs:

ğ‘ƒ
âˆ’
1
 point-to-point sends
Pâˆ’1 point-to-point sends

Time complexity:

ğ‘‚
(
ğ‘ƒ
)
O(P)
MPI_Bcast

MPI uses optimized tree-based or pipeline algorithms:

ğ‘‚
(
log
â¡
ğ‘ƒ
)
O(logP)
Theoretical Implication

As process count increases:

Manual broadcast time increases linearly.

MPI_Bcast increases logarithmically.

Conclusion

Collective operations provided by MPI are optimized and scale significantly better than manual implementations.

Q3 â€“ Distributed Dot Product
Computational Nature

Embarrassingly parallel problem.

Minimal communication.

One broadcast + one reduction.

Time Model
ğ‘‡
ğ‘
=
ğ‘‡
ğ‘
ğ‘Ÿ
ğ‘œ
ğ‘
ğ‘‘
ğ‘
ğ‘
ğ‘ 
ğ‘¡
+
ğ‘‡
ğ‘
ğ‘œ
ğ‘š
ğ‘
ğ‘¢
ğ‘¡
ğ‘’
ğ‘
+
ğ‘‡
ğ‘Ÿ
ğ‘’
ğ‘‘
ğ‘¢
ğ‘
ğ‘’
T
p
	â€‹

=T
broadcast
	â€‹

+
p
T
compute
	â€‹

	â€‹

+T
reduce
	â€‹


Since computation dominates (500 million operations), communication overhead is relatively small.

Scalability

Near-linear speedup achievable.

Eventually limited by reduction overhead and memory bandwidth.

Conclusion

Dot product represents an ideal parallel workload with strong scalability characteristics.

Q4 â€“ Prime Number Search (Dynamic Scheduling)
Workload Characteristics

Non-uniform workload.

Some numbers require more computation than others.

Static vs Dynamic Partitioning

Static partitioning â†’ load imbalance.

Dynamic masterâ€“slave â†’ better resource utilization.

Theoretical Time
ğ‘‡
ğ‘
=
ğ‘‡
ğ‘
ğ‘œ
ğ‘š
ğ‘
ğ‘¢
ğ‘¡
ğ‘’
/
ğ‘
+
ğ‘‡
ğ‘
ğ‘œ
ğ‘š
ğ‘š
ğ‘¢
ğ‘›
ğ‘–
ğ‘
ğ‘
ğ‘¡
ğ‘–
ğ‘œ
ğ‘›
+
ğ‘‡
ğ‘
ğ‘œ
ğ‘œ
ğ‘Ÿ
ğ‘‘
ğ‘–
ğ‘›
ğ‘
ğ‘¡
ğ‘–
ğ‘œ
ğ‘›
T
p
	â€‹

=T
compute
	â€‹

/p+T
communication
	â€‹

+T
coordination
	â€‹


As p increases:

Compute term decreases.

Coordination overhead increases.

Master may become bottleneck.

Conclusion

Dynamic scheduling improves load balance but introduces central coordination cost.

Q5 â€“ Perfect Number Search
Computational Nature

Divisor summation per number.

Higher computational intensity per task.

Workload irregularity.

Behavior

Similar to Q4 but with heavier per-task computation.

Scalability

Better scaling than light workloads.

Eventually limited by master communication and synchronization.

Conclusion

Masterâ€“slave architecture provides flexibility but does not scale infinitely.

4. Comparative Theoretical Insights
Problem	Compute Intensity	Communication	Scalability
Q1	Lowâ€“Moderate	High (Scatter/Gather)	Limited
Q2	Low	Communication-dominated	MPI_Bcast scales well
Q3	High	Very Low	Strong scaling
Q4	Moderate	Frequent coordination	Medium
Q5	High	Frequent coordination	Medium
5. Overall Conclusions

Scalability depends primarily on compute-to-communication ratio.

Collective MPI operations are more efficient than manual message passing.

Embarrassingly parallel problems scale better.

Masterâ€“slave models improve load balance but introduce central bottlenecks.

Amdahlâ€™s Law limits achievable speedup.

6. Graphs to Include

For a strong theoretical submission, include:

1. Execution Time vs Processes (All Problems)

Shows scalability trend.

2. Speedup vs Processes (Q1 & Q3)

Include ideal linear speedup line for comparison.

3. Manual vs MPI_Bcast Comparison (Q2)

Clearly demonstrates algorithmic complexity difference.

4. Efficiency vs Processes (Optional but recommended)

Shows diminishing returns.

7. Final Theoretical Statement

This assignment demonstrates that parallel performance is governed not only by computation division but also by communication overhead, synchronization cost, load balancing strategy, and inherent serial components of the algorithm. Effective parallel design requires minimizing communication while maximizing independent computation.
